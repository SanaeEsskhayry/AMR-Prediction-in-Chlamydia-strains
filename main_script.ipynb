{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeUfKg64gc65"
      },
      "outputs": [],
      "source": [
        "# install the necessary libraries\n",
        "! pip install numpy\n",
        "! pip install matplotlib\n",
        "! pip install -U scikit-learn\n",
        "! pip install pandas\n",
        "! pip install -U imbalanced-learn\n",
        "! pip install Bio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ8ZdQK-gX--"
      },
      "outputs": [],
      "source": [
        "# Download and install Miniconda\n",
        "!wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\n",
        "!bash miniconda.sh -bfp /usr/local\n",
        "\n",
        "# Add conda to your PATH\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages')\n",
        "\n",
        "# Update conda\n",
        "!conda update -q -y conda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFLC-wFAg_hF"
      },
      "outputs": [],
      "source": [
        "# Install the CD-HIT Algorithm\n",
        "\n",
        "! conda install -c bioconda cd-hit -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wrg4_yqgjL5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "\n",
        "#Assign the positive and negative files\n",
        "positive_file_url = \"https://raw.githubusercontent.com/SanaeEsskhayry/data/master/AMR_protein_sequences.fasta\"\n",
        "negative_file_url = \"https://raw.githubusercontent.com/SanaeEsskhayry/data/master/Sen_protein_sequences.fasta\"\n",
        "\n",
        "#Create a vairbale:filename dictionary\n",
        "url_dict = {positive_file_url:'resistant_sequences.fasta', negative_file_url:'sensitive_sequences.fasta'}\n",
        "\n",
        "#Loop through the variables in the dictionary and save the contents as filenames in the dictionary\n",
        "for url, filename in url_dict.items():\n",
        "  r = requests.get(url)\n",
        "  with open (f'{filename}', 'wb') as f:\n",
        "    f.write(r.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQYaSgRKcKWZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Function to find and return the ambiguous characters along with their positions in a protein sequence\n",
        "def find_ambiguous_characters(sequence):\n",
        "    ambiguous_pattern = re.compile(r\"[^ACDEFGHIKLMNPQRSTVWY]\")\n",
        "    matches = [(match.group(), match.start()) for match in ambiguous_pattern.finditer(sequence)]\n",
        "    return matches\n",
        "\n",
        "# Function to check if a sequence has ambiguous characters\n",
        "def has_ambiguous_characters(sequence):\n",
        "    return bool(find_ambiguous_characters(sequence))\n",
        "\n",
        "# Read the FASTA file and check for ambiguous characters\n",
        "def check_ambiguous_characters(fasta_file):\n",
        "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
        "        sequence_id = record.id\n",
        "        sequence = str(record.seq)\n",
        "        ambiguous_characters = find_ambiguous_characters(sequence)\n",
        "        if ambiguous_characters:\n",
        "            print(f\"Ambiguous characters found in sequence {sequence_id}: {sequence}\")\n",
        "            for char, position in ambiguous_characters:\n",
        "                print(f\"Ambiguous character '{char}' found at position {position}\")\n",
        "\n",
        "# Read the FASTA file and filter out sequences with ambiguous characters\n",
        "def filter_ambiguous_sequences(input_fasta, output_fasta):\n",
        "    sequences_to_keep = []\n",
        "\n",
        "    for record in SeqIO.parse(input_fasta, \"fasta\"):\n",
        "        sequence = str(record.seq)\n",
        "        if not has_ambiguous_characters(sequence):\n",
        "            sequences_to_keep.append(record)\n",
        "\n",
        "    # Write the filtered sequences to a new FASTA file\n",
        "    with open(output_fasta, \"w\") as output_file:\n",
        "        SeqIO.write(sequences_to_keep, output_file, \"fasta\")\n",
        "\n",
        "# Input and output FASTA file names\n",
        "input_fasta_file = \"sensitive_sequences.fasta\"\n",
        "output_fasta_file = \"filtered_sensitive_sequences.fasta\"\n",
        "\n",
        "# Check for ambiguous characters in the sequences\n",
        "check_ambiguous_characters(input_fasta_file)\n",
        "\n",
        "# Filter out sequences with ambiguous characters\n",
        "filter_ambiguous_sequences(input_fasta_file, output_fasta_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Function to find and return the ambiguous characters along with their positions in a protein sequence\n",
        "def find_ambiguous_characters(sequence):\n",
        "    ambiguous_pattern = re.compile(r\"[^ACDEFGHIKLMNPQRSTVWY]\")\n",
        "    matches = [(match.group(), match.start()) for match in ambiguous_pattern.finditer(sequence)]\n",
        "    return matches\n",
        "\n",
        "# Function to check if a sequence has ambiguous characters\n",
        "def has_ambiguous_characters(sequence):\n",
        "    return bool(find_ambiguous_characters(sequence))\n",
        "\n",
        "# Read the FASTA file and check for ambiguous characters\n",
        "def check_ambiguous_characters(fasta_file):\n",
        "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
        "        sequence_id = record.id\n",
        "        sequence = str(record.seq)\n",
        "        ambiguous_characters = find_ambiguous_characters(sequence)\n",
        "        if ambiguous_characters:\n",
        "            print(f\"Ambiguous characters found in sequence {sequence_id}: {sequence}\")\n",
        "            for char, position in ambiguous_characters:\n",
        "                print(f\"Ambiguous character '{char}' found at position {position}\")\n",
        "\n",
        "# Read the FASTA file and filter out sequences with ambiguous characters\n",
        "def filter_ambiguous_sequences(input_fasta, output_fasta):\n",
        "    sequences_to_keep = []\n",
        "\n",
        "    for record in SeqIO.parse(input_fasta, \"fasta\"):\n",
        "        sequence = str(record.seq)\n",
        "        if not has_ambiguous_characters(sequence):\n",
        "            sequences_to_keep.append(record)\n",
        "\n",
        "    # Write the filtered sequences to a new FASTA file\n",
        "    with open(output_fasta, \"w\") as output_file:\n",
        "        SeqIO.write(sequences_to_keep, output_file, \"fasta\")\n",
        "\n",
        "# Input and output FASTA file names\n",
        "input_fasta_file = \"resistant_sequences.fasta\"\n",
        "output_fasta_file = \"filtered_resistant_sequences.fasta\"\n",
        "\n",
        "# Check for ambiguous characters in the sequences\n",
        "check_ambiguous_characters(input_fasta_file)\n",
        "\n",
        "# Filter out sequences with ambiguous characters\n",
        "filter_ambiguous_sequences(input_fasta_file, output_fasta_file)"
      ],
      "metadata": {
        "id": "NWpB3zWKkHFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJevFE9Woru4"
      },
      "outputs": [],
      "source": [
        "#Process the files with cd-hit\n",
        "os.system(\"cd-hit -i filtered_resistant_sequences.fasta -o resistant_cdhit.txt -c 0.99\")\n",
        "os.system(\"cd-hit -i filtered_sensitive_sequences.fasta -o sensitive_cdhit.txt -c 0.99\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0Fkg1MPeYiD"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------------------\n",
        "# Features generation for resistant sequences\n",
        "# -------------------------------------------------------------------------------\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "from Bio import SeqIO\n",
        "\n",
        "fasta_file = \"resistant_cdhit.txt\"\n",
        "results = [ ]\n",
        "\n",
        "for i, record in enumerate(SeqIO.parse(fasta_file, \"fasta\")):\n",
        "\n",
        "    analysed_protein = ProteinAnalysis(record.seq)\n",
        "    # Calculate properties\n",
        "    charge_at_pH = analysed_protein.charge_at_pH(7)\n",
        "    #1-\n",
        "    molecular_weight = analysed_protein.molecular_weight()\n",
        "    #2-\n",
        "    hydrophobicity = analysed_protein.gravy()\n",
        "    #3-\n",
        "    isoelectric_point = analysed_protein.isoelectric_point()\n",
        "    #4-\n",
        "    aromaticity = analysed_protein.aromaticity()\n",
        "    #5-\n",
        "    instability_index = analysed_protein.instability_index()\n",
        "    #6-\n",
        "    secondary_structure_fraction = analysed_protein.secondary_structure_fraction()\n",
        "    #7-\n",
        "    molar_extinction_coefficient = analysed_protein.molar_extinction_coefficient()\n",
        "    #8-\n",
        "    flexibility = analysed_protein.flexibility()\n",
        "    # 9-\n",
        "    get_amino_acids_percent = analysed_protein.get_amino_acids_percent()\n",
        "\n",
        "    # Store the results\n",
        "    results.append({\n",
        "          \"Sequence\": i+1,\n",
        "          \"charge at pH =7\" : charge_at_pH,\n",
        "          \"Molecular Weight\": molecular_weight,\n",
        "          \"Hydrophobicity\": hydrophobicity,\n",
        "          \"Isoelectric Point\": isoelectric_point,\n",
        "          \"Aromaticity\": aromaticity,\n",
        "          \"Instability Index\": instability_index,\n",
        "          \"secondary structure fraction\" : secondary_structure_fraction,\n",
        "          \"Molar extinction coefficient\" : molar_extinction_coefficient,\n",
        "          \"Flexibility\" : sum(flexibility) / len(flexibility),\n",
        "          \"Amino acids percent\" : get_amino_acids_percent\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "# Display results for each sequence\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"Sequence {i + 1} Results:\")\n",
        "    for key, value in result.items():\n",
        "      print(f\"{key}: {value}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj0X_fm5es_d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Process the data to create separate columns for Amino Acids Percent, Molar Extinction Coefficient, and Secondary Structure Fraction\n",
        "for entry in results:\n",
        "    # Convert Amino Acids Percent dictionary into separate columns\n",
        "    for aa, percent in entry['Amino acids percent'].items():\n",
        "        entry[f'{aa}'] = percent\n",
        "\n",
        "    # Separate Molar Extinction Coefficient into separate columns\n",
        "    entry['MEC_reduced cysteines'], entry['MEC_disulfid bridges'] = entry['Molar extinction coefficient']\n",
        "\n",
        "    # Separate Secondary Structure Fraction into separate columns\n",
        "    entry['SSF_Helix'], entry['SSF_Turn'], entry['SSF_Sheet'] = entry['secondary structure fraction']\n",
        "\n",
        "# Convert 'results' into a DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Drop the original columns that have been separated\n",
        "df.drop(['Amino acids percent', 'Molar extinction coefficient', 'secondary structure fraction'], axis=1, inplace=True)\n",
        "\n",
        "# Print the DataFrame\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtqOnJsUghVt"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# Features generation for sensitive sequences :\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "from Bio import SeqIO\n",
        "\n",
        "fasta_file = \"sensitive_cdhit.txt\"\n",
        "results = [ ]\n",
        "\n",
        "for i, record in enumerate(SeqIO.parse(fasta_file, \"fasta\")):\n",
        "\n",
        "    analysed_protein = ProteinAnalysis(record.seq)\n",
        "    # Calculate properties\n",
        "    charge_at_pH = analysed_protein.charge_at_pH(7)\n",
        "    #1-\n",
        "    molecular_weight = analysed_protein.molecular_weight()\n",
        "    #2-\n",
        "    hydrophobicity = analysed_protein.gravy()\n",
        "    #3-\n",
        "    isoelectric_point = analysed_protein.isoelectric_point()\n",
        "    #4-\n",
        "    aromaticity = analysed_protein.aromaticity()\n",
        "    #5-\n",
        "    instability_index = analysed_protein.instability_index()\n",
        "    #6-\n",
        "    secondary_structure_fraction = analysed_protein.secondary_structure_fraction()\n",
        "    #7-\n",
        "    molar_extinction_coefficient = analysed_protein.molar_extinction_coefficient()\n",
        "    #8-\n",
        "    flexibility = analysed_protein.flexibility()\n",
        "    # 9-\n",
        "    get_amino_acids_percent = analysed_protein.get_amino_acids_percent()\n",
        "\n",
        "    # Store the results\n",
        "    results.append({\n",
        "          \"Sequence\": i+1,\n",
        "          \"charge at pH =7\" : charge_at_pH,\n",
        "          \"Molecular Weight\": molecular_weight,\n",
        "          \"Hydrophobicity\": hydrophobicity,\n",
        "          \"Isoelectric Point\": isoelectric_point,\n",
        "          \"Aromaticity\": aromaticity,\n",
        "          \"Instability Index\": instability_index,\n",
        "          \"secondary structure fraction\" : secondary_structure_fraction,\n",
        "          \"Molar extinction coefficient\" : molar_extinction_coefficient,\n",
        "          \"Flexibility\" : sum(flexibility) / len(flexibility),\n",
        "          \"Amino acids percent\" : get_amino_acids_percent\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "# Display results for each sequence\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"Sequence {i + 1} Results:\")\n",
        "    for key, value in result.items():\n",
        "      print(f\"{key}: {value}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzqgcjBmgmZw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Process the data to create separate columns for Amino Acids Percent, Molar Extinction Coefficient, and Secondary Structure Fraction\n",
        "for entry in results:\n",
        "    # Convert Amino Acids Percent dictionary into separate columns\n",
        "    for aa, percent in entry['Amino acids percent'].items():\n",
        "        entry[f'{aa}'] = percent\n",
        "\n",
        "    # Separate Molar Extinction Coefficient into separate columns\n",
        "    entry['MEC_reduced cysteines'], entry['MEC_disulfid bridges'] = entry['Molar extinction coefficient']\n",
        "\n",
        "    # Separate Secondary Structure Fraction into separate columns\n",
        "    entry['SSF_Helix'], entry['SSF_Turn'], entry['SSF_Sheet'] = entry['secondary structure fraction']\n",
        "\n",
        "# Convert 'results' into a DataFrame\n",
        "Sen_df = pd.DataFrame(results)\n",
        "\n",
        "# Drop the original columns that have been separated\n",
        "Sen_df.drop(['Amino acids percent', 'Molar extinction coefficient', 'secondary structure fraction'], axis=1, inplace=True)\n",
        "\n",
        "# Print the DataFrame\n",
        "Sen_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMKXdk0-e13m"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "# Create class labels\n",
        "Sen_class = pd.Series(['Sensitive' for i in range(len(Sen_df))])\n",
        "AMR_class = pd.Series(['Resistant' for i in range(len(df))])\n",
        "# Combine AMR Data and Sensitive Data\n",
        "AMR_Sen_class = pd.concat([Sen_class, AMR_class], axis=0)\n",
        "AMR_Sen_class.name = 'class'\n",
        "AMR_Sen_feature = pd.concat([Sen_df, df], axis=0)\n",
        "\n",
        "# Combine feature and class\n",
        "df = pd.concat([AMR_Sen_feature, AMR_Sen_class], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBKJfVTEhdxE"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALTEM12we2kp"
      },
      "outputs": [],
      "source": [
        "# Assigns the features to X and class label to Y\n",
        "X = df.drop('class', axis=1)\n",
        "y = df['class'].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SvK_3LCbDch"
      },
      "outputs": [],
      "source": [
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ6xxrVle6pv"
      },
      "outputs": [],
      "source": [
        "# Show pie plot\n",
        "y.value_counts().plot.pie(autopct='%.2f')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPPtuHRDe8yT"
      },
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "rus = RandomUnderSampler(sampling_strategy=1) # Numerical value\n",
        "# rus = RandomUnderSampler(sampling_strategy=\"not minority\") # String\n",
        "X_res, y_res = rus.fit_resample(X, y)\n",
        "\n",
        "ax = y_res.value_counts().plot.pie(autopct='%.2f')\n",
        "_ = ax.set_title(\"Under-sampling\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuCijwNyZLG6"
      },
      "outputs": [],
      "source": [
        "# Feature selection (Variance threshold)\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "fs = VarianceThreshold(threshold=0.1)\n",
        "fs.fit_transform(X_res)\n",
        "#X2.shape\n",
        "X2 = X_res.loc[:, fs.get_support()]\n",
        "X2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Epc7-vkVjvXo"
      },
      "outputs": [],
      "source": [
        "X2.shape, y_res.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VwFEIhIrjH_"
      },
      "outputs": [],
      "source": [
        "y_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5reNi6_3fCyY"
      },
      "outputs": [],
      "source": [
        "# Encoding the Y class label\n",
        "y_res = y_res.map({\"Resistant\": 1, \"Sensitive\": 0})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Features list into a csv file to be use it in the app interface:\n",
        "\n",
        "X2.to_csv('features_list.csv', index = False)"
      ],
      "metadata": {
        "id": "EboRNC9-2gE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS1_mhgnqT-Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "nan_rows = pd.isna(y_res)\n",
        "print(nan_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q_ycW6xY-0A"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize classifiers\n",
        "classifiers = [\n",
        "    ('Decision Tree', DecisionTreeClassifier()),\n",
        "    ('Random Forest', RandomForestClassifier()),\n",
        "    ('XGBoost', XGBClassifier()),\n",
        "    ('Logistic Regression', LogisticRegression(max_iter=1000)),\n",
        "    ('SVM', SVC())\n",
        "]\n",
        "\n",
        "# Print hyperparameters for each classifier\n",
        "for name, clf in classifiers:\n",
        "    print(f\"Hyperparameters for {name}:\")\n",
        "    print(clf.get_params())\n",
        "    print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tZu2Af8YB-6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X2, y_res, test_size=0.2, random_state=42, stratify=y_res)\n",
        "\n",
        "# Initialize classifiers with hyperparameters for tuning\n",
        "classifiers = [\n",
        "    ('K-Nearest Neighbors', KNeighborsClassifier(),\n",
        "     {'n_neighbors': [3, 5, 7, 9, 10 , 20, 30, 40, 50], 'weights': ['uniform', 'distance']}),\n",
        "\n",
        "    ('Decision Tree', DecisionTreeClassifier(),\n",
        "     {'max_depth': [None, 10, 20, 30, 40, 50], 'min_samples_split': [2, 5, 10,20,30]}),\n",
        "\n",
        "    ('Random Forest', RandomForestClassifier(),\n",
        "     {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}),\n",
        "\n",
        "    ('Gradient Boosting Machine', GradientBoostingClassifier(),\n",
        "     {'n_estimators': [50, 100, 200, 300], 'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5 , 1]}),\n",
        "\n",
        "    ('XGBoost', XGBClassifier(),\n",
        "     {'n_estimators': [50, 100, 200, 300], 'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5 , 1]}),\n",
        "\n",
        "    ('Logistic Regression', LogisticRegression(max_iter=1000),\n",
        "     {'C': [0.1, 1, 10 , 20, 30 , 40, 50]}),\n",
        "\n",
        "    ('Support Vector Machine', SVC(),\n",
        "     {'C': [0.1, 1, 10, 20, 30, 40, 50, 60,70]})\n",
        "]\n",
        "\n",
        "# Loop through classifiers\n",
        "for name, clf, param_grid in classifiers:\n",
        "    print(f\"Training and evaluating {name}...\")\n",
        "\n",
        "    # Hyperparameter tuning using GridSearchCV\n",
        "    grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best hyperparameters and their accuracy\n",
        "    best_params = grid_search.best_params_\n",
        "    best_accuracy = grid_search.best_score_\n",
        "    print(f\"Best Parameters: {best_params}\")\n",
        "    print(f\"Best Accuracy: {best_accuracy:.2f}\")\n",
        "\n",
        "    # Evaluate on test data\n",
        "    clf_best = grid_search.best_estimator_\n",
        "    y_pred = clf_best.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='macro')\n",
        "    recall = recall_score(y_test, y_pred, average='macro')\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    # Calculate ROC AUC for binary classification\n",
        "    if name == 'Support Vector Machine' or name == 'Logistic Regression':\n",
        "        roc_auc = roc_auc_score(y_test, clf_best.decision_function(X_test))\n",
        "    else:\n",
        "        roc_auc = roc_auc_score(y_test, clf_best.predict_proba(X_test)[:, 1])\n",
        "\n",
        "    print(f\"{name} Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"{name} Precision: {precision:.2f}\")\n",
        "    print(f\"{name} Recall: {recall:.2f}\")\n",
        "    print(f\"{name} F1-score: {f1:.2f}\")\n",
        "    print(f\"{name} AUROC: {roc_auc:.2f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# Train the best model\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X2, y_res, test_size=0.2, random_state=42)\n",
        "model = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "PSBKV5RBzEqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# Save Model as Pickle Object\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "import pickle\n",
        "with open('amr_predict_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ],
      "metadata": {
        "id": "bme8cGrjwUA3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}